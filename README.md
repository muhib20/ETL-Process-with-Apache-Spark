# ETL-Process-with-Apache-Spark
## Description
This project demonstrates an ETL process using Apache Spark. The ETL process involves extracting data from various sources, transforming it as needed, and loading the processed data into a target storage system, which in this case is the Hadoop Distributed File System (HDFS).

## Installation
To run this project, follow these steps:

1. Install Apache Spark on your Linux server.
2. Set up a data storage system like Hadoop Distributed File System (HDFS).
3. Ensure you have a programming language like Python or Scala installed for Spark development.

## Project Details
- **Extract**: Data is extracted from source systems, including CSV files and databases.
- **Transform**: Data undergoes various transformations, such as cleaning, aggregating, or joining, as required by the business logic.
- **Load**: Processed data is loaded into the target storage system, which in this case is HDFS.

## Architecture Overview
![ETL Process with Apache Spark Architecture diagram](https://github.com/muhib20/ETL-Process-with-Apache-Spark/assets/60944550/edecc677-7430-4502-8b63-f371b4868674)

## Usage
For usage please check the Configuration files

## Contact
contact@muhibhafeez.com
